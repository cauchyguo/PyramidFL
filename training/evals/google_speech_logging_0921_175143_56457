2023-09-21:17:51:53,360 INFO     [param_server.py:13] End up with cuda device tensor([0.3586], device='cuda:0')
2023-09-21:17:51:54,17 INFO     [param_server.py:616] ====Start to initialize dataset
2023-09-21:17:51:54,18 INFO     [flLibs.py:74] ====Initialize the model
2023-09-21:17:52:23,420 INFO     [learner.py:13] End up with cuda device tensor([0.2977], device='cuda:1')
2023-09-21:17:52:23,420 INFO     [learner.py:41] ===== Experiment start on : user-Server=====
2023-09-21:17:52:23,626 INFO     [learner.py:13] End up with cuda device tensor([0.6259], device='cuda:3')
2023-09-21:17:52:23,626 INFO     [learner.py:41] ===== Experiment start on : user-Server=====
2023-09-21:17:52:23,958 INFO     [learner.py:13] End up with cuda device tensor([0.1262], device='cuda:2')
2023-09-21:17:52:23,959 INFO     [learner.py:41] ===== Experiment start on : user-Server=====
2023-09-21:17:52:23,993 INFO     [learner.py:709] ====Start to initialize dataset
2023-09-21:17:52:23,993 INFO     [flLibs.py:74] ====Initialize the model
2023-09-21:17:52:24,205 INFO     [learner.py:709] ====Start to initialize dataset
2023-09-21:17:52:24,205 INFO     [flLibs.py:74] ====Initialize the model
2023-09-21:17:52:24,477 INFO     [learner.py:709] ====Start to initialize dataset
2023-09-21:17:52:24,477 INFO     [flLibs.py:74] ====Initialize the model
2023-09-21:17:52:24,650 INFO     [learner.py:13] End up with cuda device tensor([0.9224], device='cuda:0')
2023-09-21:17:52:24,651 INFO     [learner.py:41] ===== Experiment start on : user-Server=====
2023-09-21:17:52:25,129 INFO     [learner.py:709] ====Start to initialize dataset
2023-09-21:17:52:25,129 INFO     [flLibs.py:74] ====Initialize the model
2023-09-21:17:52:36,692 INFO     [distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-21:17:52:37,286 INFO     [distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-21:17:52:37,692 INFO     [distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-21:17:52:38,961 INFO     [distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-21:17:52:38,968 INFO     [distributed_c10d.py:319] Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-21:17:52:38,970 INFO     [distributed_c10d.py:354] Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-09-21:17:52:38,972 INFO     [distributed_c10d.py:354] Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-09-21:17:52:38,972 INFO     [learner.py:732] ==== Starting training data partitioner =====
2023-09-21:17:52:38,976 INFO     [distributed_c10d.py:354] Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-09-21:17:52:38,977 INFO     [learner.py:732] ==== Starting training data partitioner =====
2023-09-21:17:52:38,977 INFO     [distributed_c10d.py:354] Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-09-21:17:52:38,977 INFO     [distributed_c10d.py:354] Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
2023-09-21:17:52:38,978 INFO     [learner.py:732] ==== Starting training data partitioner =====
2023-09-21:17:52:38,985 INFO     [learner.py:732] ==== Starting training data partitioner =====
2023-09-21:17:52:39,61 INFO     [divide_data.py:95] ====Initiating DataPartitioner takes 0.08277463912963867 s

2023-09-21:17:52:39,61 INFO     [learner.py:735] ==== Finished training data partitioner =====
2023-09-21:17:52:39,82 INFO     [divide_data.py:95] ====Initiating DataPartitioner takes 0.1095728874206543 s

2023-09-21:17:52:39,83 INFO     [learner.py:735] ==== Finished training data partitioner =====
2023-09-21:17:52:39,96 INFO     [divide_data.py:95] ====Initiating DataPartitioner takes 0.1108100414276123 s

2023-09-21:17:52:39,96 INFO     [learner.py:735] ==== Finished training data partitioner =====
2023-09-21:17:52:39,121 INFO     [divide_data.py:95] ====Initiating DataPartitioner takes 0.14342427253723145 s

2023-09-21:17:52:39,121 INFO     [learner.py:735] ==== Finished training data partitioner =====
2023-09-21:17:52:44,786 INFO     [param_server.py:98] ====Info of all feasible clients {'total_feasible_clients': 491, 'total_length': 33443}
Traceback (most recent call last):
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 745, in <module>
    report_data_info(this_rank, q)
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 95, in report_data_info
    dist.broadcast(tensor=clientIdToRun, src=0)
  File "/home/ypguo/.conda/envs/mytorch/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 1400, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1269, internal error, NCCL version 2.14.3
ncclInternalError: Internal check failed.
Last error:
Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 18000
Traceback (most recent call last):
  File "/disk7T/ypguo/PyramidFL/training/param_server.py", line 624, in <module>
    q, param_q, stop_signal, run, args.backend
  File "/disk7T/ypguo/PyramidFL/training/param_server.py", line 127, in init_myprocesses
    dist.broadcast(tensor=clientTensor, src=0)
  File "/home/ypguo/.conda/envs/mytorch/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 1400, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1269, internal error, NCCL version 2.14.3
ncclInternalError: Internal check failed.
Last error:
Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 18000
Traceback (most recent call last):
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 745, in <module>
    report_data_info(this_rank, q)
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 95, in report_data_info
    dist.broadcast(tensor=clientIdToRun, src=0)
  File "/home/ypguo/.conda/envs/mytorch/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 1400, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1269, internal error, NCCL version 2.14.3
ncclInternalError: Internal check failed.
Last error:
Net : Call to recv from 192.168.1.106<52603> failed : Broken pipe
Traceback (most recent call last):
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 745, in <module>
    report_data_info(this_rank, q)
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 95, in report_data_info
    dist.broadcast(tensor=clientIdToRun, src=0)
  File "/home/ypguo/.conda/envs/mytorch/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 1400, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1269, internal error, NCCL version 2.14.3
ncclInternalError: Internal check failed.
Last error:
Net : Call to recv from 192.168.1.106<59099> failed : Broken pipe
Traceback (most recent call last):
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 745, in <module>
    report_data_info(this_rank, q)
  File "/disk7T/ypguo/PyramidFL/training/learner.py", line 95, in report_data_info
    dist.broadcast(tensor=clientIdToRun, src=0)
  File "/home/ypguo/.conda/envs/mytorch/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 1400, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError
